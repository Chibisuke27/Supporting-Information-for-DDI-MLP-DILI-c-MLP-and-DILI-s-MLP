{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import Python Libraries into the Jupyter Notebook.\n",
    "\n",
    "Note: Please download the relevant Python packages to execute this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem.Fingerprints import FingerprintMols\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "import pandas as pd\n",
    "from rdkit.Chem.rdMolDescriptors import GetMorganFingerprint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Load the csv file that contains the drug molecules as represented by their DB_ID and SMILES. DB_ID refers to the DrugBank ID that is used to unequivocally refer to a specific drug structure in the DrugBank database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_df=pd.read_csv(\"1_1_1_Input\\DB_ID_SMILES.csv\")\n",
    "fixed_db_id=fixed_df.loc[:,\"DB_ID\"].values\n",
    "fixed_SMI=fixed_df.loc[:,\"SMILES_Representation\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Generate the 1,710x1,710 Tanimoto Correlation Matrix using Morgan2 Fingerprint in the RDKit library as pandas DataFrame called \"Tanimoto_Correlation_Matrix_1710_x_1710\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_fixed_SMI=list(fixed_SMI)\n",
    "fixed_mol=[]\n",
    "for i in range(len(list_fixed_SMI)):\n",
    "    fixed_mol.append(Chem.MolFromSmiles(list_fixed_SMI[i]))\n",
    "\n",
    "DDI_mol=fixed_mol\n",
    "\n",
    "DDI_fps=[]\n",
    "for index, i in enumerate(DDI_mol):\n",
    "    try:\n",
    "        DDI_fps.append(AllChem.GetMorganFingerprint(i, 2))\n",
    "    except:\n",
    "        print(index, i)\n",
    "        \n",
    "fixed_fps=DDI_fps\n",
    "\n",
    "Tanimoto_List=[]\n",
    "for i in range(len(DDI_fps)):\n",
    "    temp=[]\n",
    "    for a in range(len(fixed_fps)):\n",
    "        temp.append(str(DataStructs.TanimotoSimilarity(DDI_fps[i], fixed_fps[a])))\n",
    "    Tanimoto_List.append(temp)\n",
    "    \n",
    "df_Tanimoto=pd.DataFrame(data=Tanimoto_List, columns=fixed_db_id)\n",
    "Complementary_TC_df=pd.DataFrame(fixed_db_id, columns=['Correlation Matrix'])\n",
    "\n",
    "Tanimoto_Correlation_Matrix_1710_x_1710 = pd.concat([Complementary_TC_df, df_Tanimoto], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Reduce Tanimoto_Correlation_Matrix_1710_x_1710 to SSP_1710x50 using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_n_factor(standardized_x, threshold):\n",
    "    #Step 4: Factor Extraction by Performing PCA on Standardized Data\n",
    "    pca=PCA()\n",
    "    principalComponents=pca.fit_transform(standardized_x)\n",
    "    eigenvalue_pca=pca.explained_variance_\n",
    "    #print(eigenvalue_pca)\n",
    "    for i in range(len(eigenvalue_pca)):\n",
    "        if float(eigenvalue_pca[i])<threshold:\n",
    "            output=i\n",
    "            break\n",
    "    PCA_component=pca.explained_variance_ratio_\n",
    "    loadings = pca.components_\n",
    "    return i, eigenvalue_pca, principalComponents, PCA_component, loadings\n",
    "\n",
    "DB_ID=list(Tanimoto_Correlation_Matrix_1710_x_1710.loc[:,\"Correlation Matrix\"].values)\n",
    "fixed_comparison_features=list(Tanimoto_Correlation_Matrix_1710_x_1710.columns.values)[1:]\n",
    "x=Tanimoto_Correlation_Matrix_1710_x_1710.loc[:, fixed_comparison_features].values\n",
    "standardized_x=StandardScaler().fit_transform(x)\n",
    "df_standardized=pd.DataFrame(data=standardized_x, columns=fixed_comparison_features)\n",
    "\n",
    "n_pca=len(x)\n",
    "full_eigenvalue_pca, full_principal_components, variance_explained_per_PC, loadings=pca_n_factor(standardized_x,0)[1:]\n",
    "\n",
    "######\n",
    "columns=[]\n",
    "for i in range(n_pca):\n",
    "    temp_line='PC-'+str(i+1)\n",
    "    columns.append(temp_line)\n",
    "principalDf=pd.DataFrame(data=full_principal_components, columns=columns)\n",
    "######\n",
    "\n",
    "DB_ID_Df=pd.DataFrame(DB_ID, columns=[\"DB_ID\"])\n",
    "Concatenated_TC_df = pd.concat([DB_ID_Df, principalDf], axis=1)\n",
    "\n",
    "SSP_1710x50=Concatenated_TC_df[list(Concatenated_TC_df.columns)[0:51]]\n",
    "SSP_1710x50.to_csv(\"1_1_2_Output\\DDI_MLP_SSP.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
